{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to deep learning using Keras\n",
    "\n",
    "In this class we will explore some of the deep learning capabilities of the library Keras. For this, we will use the CIFAR-10 dataset\n",
    "\n",
    "<img src=\"cifar_10.png\" />\n",
    "\n",
    "\n",
    "CIFAR-10, contains 32×32×3 coloured images: if we are to treat each channel of each pixel as an independent input to an MLP, each neuron of the first hidden layer adds ∼ 3000 new parameters to the model! The situation quickly becomes unmanageable as image sizes grow larger, way before reaching the kind of images people usually want to work with in real applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Lets start by describing what is a convolutional neural network, an architecture that has achieved state of the art performance for image recognition.\n",
    "\n",
    "## Convolutions\n",
    "\n",
    "Convolutional neural networks (CNNs) are a specialized kind of neural network \n",
    "for processing data which has spatial correlation between the neighborhood data points which \n",
    "is also called as grid-like topology. \n",
    "\n",
    "<p>Enter the <em>convolution</em> operator. Given a two-dimensional image, $I$, and a small matrix, $K$ of size $h \\times w$, (known as a <em>convolution kernel</em>), which we assume encodes a way of extracting an interesting image feature, we compute the convolved image, $I * K$, by overlaying the kernel on top of the image in all possible ways, and recording the sum of elementwise products between the image and the kernel:</p>\n",
    "$$(I * K)_{xy} = \\sum_{i=1}^h \\sum_{j=1}^w {K_{ij} \\cdot I_{x + i - 1, y + j - 1}}$$\n",
    "\n",
    "<p>The images below show a diagrammatical overview of the above formula and the result of applying convolution (with two separate kernels) over an image, to act as an edge detector:</p>\n",
    "\n",
    "![](convolve.png)\n",
    "\n",
    "![](lena.jpg)\n",
    "\n",
    "# Activation\n",
    "\n",
    "May different activation functions are possible, but ReLUs are by far the most popular ones.\n",
    "\n",
    "\n",
    "# Pooling\n",
    "\n",
    "\n",
    "After application of the activation function, programmers may choose to apply a pooling layer. It is also referred to as a downsampling layer. In this category, there are also several layer options, with maxpooling being the most popular. This basically takes a filter (normally of size 2x2) and a stride of the same length. It then applies it to the input volume and outputs the maximum number in every subregion that the filter convolves around.\n",
    "\n",
    "![](max_pool.png)\n",
    "\n",
    "For a real example (note that the z dimension, the number of layers, remains unchanged in the pooling operation):\n",
    "\n",
    "![](max_pooling_2.jpeg)\n",
    "\n",
    "# Combining them into a neural network\n",
    "\n",
    "The different convolution and pooling layers are then combined into a single architecture by composition. The last layer is different as it needs to generate a multi-class decision. It is commonly a softmax layer.\n",
    "\n",
    "![](architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps with Keras\n",
    "\n",
    "Keras is a high-level library for deep learning with an API modeled after scikit-learn. It makes use of Theano of Tensorflow beneath the scenes for the actual computations.\n",
    "\n",
    "Keras can be installed from conda-forge. Uncomment and run the next cell to install using conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata .............\n",
      "Solving package specifications: .\n",
      "\n",
      "# All requested packages already installed.\n",
      "# packages in environment at /home/fabian/anaconda3:\n",
      "#\n",
      "keras                     2.0.6                    py36_0    conda-forge\n",
      "tensorflow                1.3.0                    py36_0    conda-forge\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes --channel https://conda.anaconda.org/conda-forge keras tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential # basic class for specifying and training a neural network\n",
    "from keras.datasets import cifar10 # subroutines for fetching the CIFAR-10 dataset\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dense, Dropout, Flatten, Activation\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The CNN relies on several hyperparameters. For the purposes of this tutorial, we will also stick to \"sensible\" hand-picked values for them, but do still keep in mind that later on I will introduce a more proper method for learning them:\n",
    "\n",
    " * The batch size, representing the number of training examples being used simultaneously during a single iteration of the gradient descent algorithm.\n",
    " * The number of epochs, representing the number of times the training algorithm will iterate over the entire training set before terminating.\n",
    " * The kernel sizes in the convolutional layers.\n",
    " * The pooling size in the pooling layers.\n",
    " * The number of kernels in the convolutional layers.\n",
    " * The dropout probability (we will apply dropout after each pooling, and after the fully connected layer).\n",
    " * The number of neurons in the fully connected layer of the MLP.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32   # in each iteration, we consider 32 training examples at once\n",
    "num_epochs = 10   # we iterate 10 times over the entire training set\n",
    "kernel_size = 3   # we will use 3x3 kernels throughout\n",
    "pool_size = 2     # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 20 # we will initially have 20 kernels per conv. layer...\n",
    "conv_depth_2 = 50 # ...switching to 50 after the first pooling layer\n",
    "hidden_size = 512 # the FC layer will have 512 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "num_classes = np.unique(y_train).size\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The LeNet architecture\n",
    "\n",
    "![](lenet.png)\n",
    "\n",
    "The LeNet architecture is an classical Convolutional Neural Networks architecture.\n",
    "\n",
    "LeNet is small and easy to understand — yet large enough to provide interesting results. Furthermore, it is able to run on the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(conv_depth_1, (kernel_size, kernel_size), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(conv_depth_2, (kernel_size, kernel_size), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the model on the data.\n",
    "\n",
    "<img style=\"float: left; width: 50px; top: -20px\" src=\"https://cdn1.iconfinder.com/data/icons/hawcons/32/700303-icon-61-warning-128.png\" /> In Keras, before a model is fitted it needs to be \"compiled\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# we need to choose the optimization method that we will use\n",
    "# initiate RMSprop optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Let's train the model using RMSprop\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=opt,\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 61s - loss: 1.5424 - acc: 0.4479 - val_loss: 1.3465 - val_acc: 0.5225\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 56s - loss: 1.2118 - acc: 0.5734 - val_loss: 1.1170 - val_acc: 0.6215\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 55s - loss: 1.1231 - acc: 0.6110 - val_loss: 1.1708 - val_acc: 0.6015\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 56s - loss: 1.0940 - acc: 0.6257 - val_loss: 1.0854 - val_acc: 0.6441\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 55s - loss: 1.0842 - acc: 0.6327 - val_loss: 1.2338 - val_acc: 0.5899\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 56s - loss: 1.0825 - acc: 0.6405 - val_loss: 1.2061 - val_acc: 0.6041\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 59s - loss: 1.0998 - acc: 0.6324 - val_loss: 1.0675 - val_acc: 0.6442\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 55s - loss: 1.1119 - acc: 0.6313 - val_loss: 1.1824 - val_acc: 0.6050\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 56s - loss: 1.1238 - acc: 0.6296 - val_loss: 1.3727 - val_acc: 0.5694\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 55s - loss: 1.1359 - acc: 0.6286 - val_loss: 1.2677 - val_acc: 0.5694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc46a7e0cf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              validation_data=(x_test, y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s     \n",
      "Test loss: 1.26771278038\n",
      "Test accuracy: 0.5694\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn <=> Keras dictionary\n",
    "\n",
    "```model.fit```  <-> ```model.compile``` followed by ```model.fit```\n",
    "\n",
    "```model.predict``` <-> ```model.predict```\n",
    "\n",
    "```model.score``` <-> ```model.evaluate(x)[1]```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn  <=> Keras interoperability\n",
    "\n",
    "It is possible to convert a Keras classifier into a scikit-learn estimator so it can be used in objects such as ```GridSearchCV``` or ```Pipeline```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/fabian/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "33333/33333 [==============================] - 14s - loss: 2.0915 - acc: 0.2176    \n",
      "Epoch 2/3\n",
      "33333/33333 [==============================] - 15s - loss: 1.8199 - acc: 0.3315    \n",
      "Epoch 3/3\n",
      "33333/33333 [==============================] - 15s - loss: 1.7212 - acc: 0.3663    \n",
      "16512/16667 [============================>.] - ETA: 0sEpoch 1/3\n",
      "33333/33333 [==============================] - 15s - loss: 2.2597 - acc: 0.1304    \n",
      "Epoch 2/3\n",
      "33333/33333 [==============================] - 15s - loss: 2.0223 - acc: 0.2303    \n",
      "Epoch 3/3\n",
      "33333/33333 [==============================] - 15s - loss: 1.8967 - acc: 0.2932    \n",
      "16544/16667 [============================>.] - ETA: 0sEpoch 1/3\n",
      "33334/33334 [==============================] - 15s - loss: 2.1379 - acc: 0.2047    \n",
      "Epoch 2/3\n",
      "33334/33334 [==============================] - 15s - loss: 1.8960 - acc: 0.3039    \n",
      "Epoch 3/3\n",
      "33334/33334 [==============================] - 15s - loss: 1.7794 - acc: 0.3513    \n",
      "16640/16666 [============================>.] - ETA: 0sEpoch 1/6\n",
      "33333/33333 [==============================] - 15s - loss: 1.9857 - acc: 0.2627    \n",
      "Epoch 2/6\n",
      "33333/33333 [==============================] - 15s - loss: 1.7935 - acc: 0.3426    \n",
      "Epoch 3/6\n",
      "33333/33333 [==============================] - 15s - loss: 1.7312 - acc: 0.3640    \n",
      "Epoch 4/6\n",
      "33333/33333 [==============================] - 15s - loss: 1.6798 - acc: 0.3839    \n",
      "Epoch 5/6\n",
      "33333/33333 [==============================] - 14s - loss: 1.6519 - acc: 0.3943    \n",
      "Epoch 6/6\n",
      "33333/33333 [==============================] - 15s - loss: 1.6260 - acc: 0.4063    \n",
      "16320/16667 [============================>.] - ETA: 0sEpoch 1/6\n",
      "33333/33333 [==============================] - 15s - loss: 2.0091 - acc: 0.2556    \n",
      "Epoch 2/6\n",
      "33333/33333 [==============================] - 14s - loss: 1.7500 - acc: 0.3565    \n",
      "Epoch 3/6\n",
      "33333/33333 [==============================] - 15s - loss: 1.6663 - acc: 0.3887    \n",
      "Epoch 4/6\n",
      "33333/33333 [==============================] - 15s - loss: 1.6212 - acc: 0.4088    \n",
      "Epoch 5/6\n",
      "33333/33333 [==============================] - 15s - loss: 1.5891 - acc: 0.4222    \n",
      "Epoch 6/6\n",
      "33333/33333 [==============================] - 14s - loss: 1.5594 - acc: 0.4341    \n",
      "16640/16667 [============================>.] - ETA: 0sEpoch 1/6\n",
      "33334/33334 [==============================] - 14s - loss: 2.1593 - acc: 0.1810    \n",
      "Epoch 2/6\n",
      "33334/33334 [==============================] - 15s - loss: 1.9274 - acc: 0.2816    \n",
      "Epoch 3/6\n",
      "33334/33334 [==============================] - 14s - loss: 1.8148 - acc: 0.3319    \n",
      "Epoch 4/6\n",
      "33334/33334 [==============================] - 15s - loss: 1.7348 - acc: 0.3650    \n",
      "Epoch 5/6\n",
      "33334/33334 [==============================] - 14s - loss: 1.6806 - acc: 0.3829    \n",
      "Epoch 6/6\n",
      "33334/33334 [==============================] - 15s - loss: 1.6444 - acc: 0.3987    \n",
      "16416/16666 [============================>.] - ETA: 0sEpoch 1/3\n",
      "33333/33333 [==============================] - 14s - loss: 1.9119 - acc: 0.2939    \n",
      "Epoch 2/3\n",
      "33333/33333 [==============================] - 15s - loss: 1.6760 - acc: 0.3937    \n",
      "Epoch 3/3\n",
      "33333/33333 [==============================] - 14s - loss: 1.5854 - acc: 0.4271    \n",
      "16667/16667 [==============================] - 2s     \n",
      "Epoch 1/3\n",
      "33333/33333 [==============================] - 15s - loss: 1.9599 - acc: 0.2839    \n",
      "Epoch 2/3\n",
      "33333/33333 [==============================] - 14s - loss: 1.7292 - acc: 0.3722    \n",
      "Epoch 3/3\n",
      "33333/33333 [==============================] - 15s - loss: 1.6614 - acc: 0.3990    \n",
      "16416/16667 [============================>.] - ETA: 0sEpoch 1/3\n",
      "33334/33334 [==============================] - 15s - loss: 1.9229 - acc: 0.2842    \n",
      "Epoch 2/3\n",
      "33334/33334 [==============================] - 14s - loss: 1.6689 - acc: 0.3874    \n",
      "Epoch 3/3\n",
      "33334/33334 [==============================] - 15s - loss: 1.5750 - acc: 0.4288    \n",
      "16480/16666 [============================>.] - ETA: 0sEpoch 1/6\n",
      "33333/33333 [==============================] - 15s - loss: 2.1538 - acc: 0.1998    \n",
      "Epoch 2/6\n",
      "33333/33333 [==============================] - 15s - loss: 1.8402 - acc: 0.3302    \n",
      "Epoch 3/6\n",
      "33333/33333 [==============================] - 14s - loss: 1.7161 - acc: 0.3715    \n",
      "Epoch 4/6\n",
      "33333/33333 [==============================] - 16s - loss: 1.6495 - acc: 0.4037    \n",
      "Epoch 5/6\n",
      "33333/33333 [==============================] - 16s - loss: 1.6085 - acc: 0.4214    \n",
      "Epoch 6/6\n",
      "33333/33333 [==============================] - 14s - loss: 1.5613 - acc: 0.4394    \n",
      "16384/16667 [============================>.] - ETA: 0sEpoch 1/6\n",
      "33333/33333 [==============================] - 16s - loss: 2.0064 - acc: 0.2718    \n",
      "Epoch 2/6\n",
      "33333/33333 [==============================] - 16s - loss: 1.7373 - acc: 0.3772    \n",
      "Epoch 3/6\n",
      "33333/33333 [==============================] - 15s - loss: 1.6344 - acc: 0.4135    \n",
      "Epoch 4/6\n",
      "33333/33333 [==============================] - 15s - loss: 1.5695 - acc: 0.4386    \n",
      "Epoch 5/6\n",
      "33333/33333 [==============================] - 15s - loss: 1.5317 - acc: 0.4516    \n",
      "Epoch 6/6\n",
      "33333/33333 [==============================] - 14s - loss: 1.5010 - acc: 0.4613    \n",
      "16352/16667 [============================>.] - ETA: 0sEpoch 1/6\n",
      "33334/33334 [==============================] - 14s - loss: 1.9827 - acc: 0.2677    \n",
      "Epoch 2/6\n",
      "33334/33334 [==============================] - 15s - loss: 1.7185 - acc: 0.3783    \n",
      "Epoch 3/6\n",
      "33334/33334 [==============================] - 15s - loss: 1.6243 - acc: 0.4139    \n",
      "Epoch 4/6\n",
      "33334/33334 [==============================] - 14s - loss: 1.5660 - acc: 0.4381    \n",
      "Epoch 5/6\n",
      "33334/33334 [==============================] - 14s - loss: 1.5234 - acc: 0.4531    \n",
      "Epoch 6/6\n",
      "33334/33334 [==============================] - 15s - loss: 1.4931 - acc: 0.4655    \n",
      "16416/16666 [============================>.] - ETA: 0sEpoch 1/3\n",
      "33333/33333 [==============================] - 14s - loss: 2.0389 - acc: 0.2397    \n",
      "Epoch 2/3\n",
      "33333/33333 [==============================] - 15s - loss: 1.7549 - acc: 0.3528    \n",
      "Epoch 3/3\n",
      "26720/33333 [=======================>......] - ETA: 3s - loss: 1.6618 - acc: 0.3912"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def make_model(dense_layer_sizes, filters, kernel_size, pool_size):\n",
    "    '''Creates model comprised of 2 convolutional layers followed by dense layers\n",
    "\n",
    "    dense_layer_sizes: List of layer sizes.\n",
    "        This list has one number for each layer\n",
    "    filters: Number of convolutional filters in each convolutional layer\n",
    "    kernel_size: Convolutional kernel size\n",
    "    pool_size: Size of pooling area for max pooling\n",
    "    '''\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters, kernel_size,\n",
    "                     padding='valid',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(filters, kernel_size))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    for layer_size in dense_layer_sizes:\n",
    "        model.add(Dense(layer_size))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "dense_size_candidates = [[32], [64], [32, 32], [64, 64]]\n",
    "my_classifier = KerasClassifier(make_model, batch_size=32)\n",
    "validator = GridSearchCV(my_classifier,\n",
    "                         param_grid={'dense_layer_sizes': dense_size_candidates,\n",
    "                                     # epochs is avail for tuning even when not\n",
    "                                     # an argument to model building function\n",
    "                                     'epochs': [3, 6],\n",
    "                                     'filters': [8],\n",
    "                                     'kernel_size': [3],\n",
    "                                     'pool_size': [2]},\n",
    "                         scoring='neg_log_loss',\n",
    "                         n_jobs=1)\n",
    "validator.fit(x_train, y_train)\n",
    "\n",
    "print('The parameters of the best model are: ')\n",
    "print(validator.best_params_)\n",
    "\n",
    "# validator.best_estimator_ returns sklearn-wrapped version of best model.\n",
    "# validator.best_estimator_.model returns the (unwrapped) keras model\n",
    "best_model = validator.best_estimator_.model\n",
    "metric_names = best_model.metrics_names\n",
    "metric_values = best_model.evaluate(x_test, y_test)\n",
    "for metric, value in zip(metric_names, metric_values):\n",
    "    print(metric, ': ', value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chihuahua vs Muffin\n",
    "\n",
    "This is one of the most important and challenging problems of our time. For decades, worlds best minds have tried and failed. Will you be able to build a classifier to distinguish chihuahua from a muffin?\n",
    "\n",
    "\n",
    "![](chihuahua_vs_muffin.jpg)\n",
    "\n",
    "For this, I have prepared a dataset consisting of 100 train images of chihuahuas and muffins, 100 for the test set and 100 for the validation set that are in the current directory. You should extract it to the current directory (i.e. wherever this notebook lives).\n",
    "\n",
    "In this case, since the dataset is quite big (images are larger). To avoid loading it into memory, we will use the ```ImageGenerator``` object to create a ```train_generator``` that will be passed to the model, except the model will be trained using ```fit_generator``` instead of ```fit```.\n",
    "\n",
    "Note: the [```ImageGenerator```](https://keras.io/preprocessing/image/) is extremely versatile and has many useful features. Get to know it.\n",
    "\n",
    "**Goal of the exercise**. Create a classifier that distinguishes Chihuahua vs Muffin. Compute and report score on validation set.\n",
    "\n",
    "Hint: you have few data. You might want to do some [data augmentation](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'train',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'test',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        'validation',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
